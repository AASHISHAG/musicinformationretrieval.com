{
 "metadata": {
  "name": "",
  "signature": "sha256:5f55a85c179da24c955d98d1e953b0090994106b925bdb1f7151663233c3cf61"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write a script to list which audio slices (or audio files) were categorized as Cluster # 1.  Do the same or Cluster # 2.  Do the clusters make sense?    Now, modify the script to play the audio slices that in each cluster - listening to the clusters will help us build intuition of what's in each cluster.  \n",
      "\n",
      "Repeat this clustering (steps 3-7), and listening to the contents of the clusters with CongaGroove-mono.wav.   \n",
      "\n",
      "Repeat this clustering (steps 3-7) using the CongaGroove and 3 clusters.  Listen to the results.  Try again with 4 clusters.  Listen to the results.  (etc, etc\u2026)\n",
      "\n",
      "Once you complete this, try out some of the many, many other audio loops in the audio loops. (Located In audio\\Miscellaneous Loops Samples and SFX)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's add MFCCs to the mix.  Extract the mean of the 12 MFCCs (coefficients 1-12, do not use the \"0th\" coefficient) for each onset using the code that you wrote.  Add those to the feature vectors, along with zero crossing and centroid.  We should now have 14 features being extracted - this is started to get \"real world\"!   With this simple example (and limited collection of audio slices, you probably won't notice a difference - but at least it didn't break, right?)  Let's try it with the some other audio to truly appreciate the power of timbral clustering."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "BONUS (ONLY IF YOU HAVE EXTRA TIME\u2026)\n",
      "Now that we can take ANY LOOP, onset detect, feature extract, and cluster it, let's have some fun.   \n",
      "Choose any audio file from our collection and use the above techniques break it up into clusters.   \n",
      "Listen to those clusters.\n",
      "\n",
      "Some rules of thumb: since you need to pick the number of clusters ahead of time, listen to your audio files first.  \n",
      "You can break a drum kit or percussion loop into 3 - 6 clusters for it to segment well.  More is OK too.\n",
      "Musical loops: 3-6 clusters should work nicely.  \n",
      "Songs - lots of clusters for them to segment well.  Try 'em out!\n",
      "\n",
      "BONUS (ONLY IF YOU REALLY HAVE EXTRA TIME\u2026)\n",
      "Review your script that PLAYs all of the audio files that were categorized as Cluster # 1 or Cluster # 2.  \n",
      "Now, modify your script to play and plot the audio files which are closest to the center of your clusters.\n",
      "\n",
      "This hopefully provides you with which files are representative of your cluster.  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}