{
 "metadata": {
  "name": "",
  "signature": "sha256:fac5244635d1c30aeb93ebb9a63a2dd73149e822761ae77c22152ff7a2bf9170"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab 2\n",
      "=====\n",
      "\n",
      "Purpose: To gain an understanding of feature extraction, windowing, MFCCs.\n",
      "\n",
      "SECTION 1 SEGMENTING INTO EVERY N ms FRAMES\n",
      "-------------------------------------------\n",
      "\n",
      "Segmenting: Chopping up into frames every N seconds\n",
      "\n",
      "Previously, we've either chopped up a signal by the location of it's onsets (and taking the following 100 ms) or just analyzing the entire file. \n",
      "Analyzing the audio file by \"frames\" is another technique for your arsenal that is good for analyzing entire songs, phrases, or non-onset-based audio examples.\n",
      "You easily chop up the audio into frames every, say, 100ms, with a for loop. \n",
      "\n",
      "    frameSize = 0.100 * fs; % 100ms\n",
      "    for i = 1: frameSize : (length(x)-frameSize+1) \n",
      "        currentFrame = x(i:i+frameSize-1); % this is the current audio frame \n",
      "        % Now, do your feature extraction here and store the features in some matrix / array\n",
      "    end\n",
      "\n",
      "Very often, you will want to have some overlap between the audio frames - taking an 100ms long frame but sliding it 50 ms each time. To do a 100ms frame and have it with 50% overlap, try: \n",
      "\n",
      "    frameSize = 0.100 * fs; % 100ms\n",
      "    hop = 0.5; % 50%overlap\n",
      "    for i = 1: hop * frameSize : (length(x)-frameSize+1) \n",
      "        ...\n",
      "    end\n",
      "\n",
      "Note that it's also important to multiple the signal by a window (e.g., Hamming / Hann window) equal to the frame size to smoothly transition between the frames. \n",
      "\n",
      "SECTION 2 MFCC\n",
      "--------------\n",
      "\n",
      "Load an audio file of your choosing from the audio folder on `/usr/ccrma/courses/mir2012/audio`.\n",
      "Use this as an opportunity to explore this collection.\n",
      "\n",
      "BAG OF FRAMES\n",
      "\n",
      "Test out MFCC to make sure that you know how to call it. We'll use the CATbox implementation of MFCC.\n",
      "\n",
      "    currentFrameIndex = 1; \n",
      "    for i = 1: frameSize : (length(x)-frameSize+1)\n",
      "        currentFrame = x(i:i+frameSize-1) + eps ; % this is the current audio frame\n",
      "        % Note that we add EPS to prevent divide by 0 errors % Now, do your other feature extraction here \n",
      "        % The code generates MFCC coefficients for the audio signal given in the current frame.\n",
      "        [mfceps] = mfcc(currentFrame ,fs)' ; %note the transpose operator!\n",
      "        delta_mfceps = mfceps - [zeros(1,size(mfceps,2)); mfceps(1:end-1,:)]; %first delta\n",
      "        % Calculate the mean and std of the MFCCs, MFCC-deltas.\n",
      "        MFCC_mean(currentFrameIndex,:) = mean(mfceps);\n",
      "        MFCC_std(currentFrameIndex,:) = std(mfceps);\n",
      "        MFCC_delta_mean (currentFrameIndex,:)= mean(delta_mfceps);\n",
      "        MFCC_delta_std(currentFrameIndex,:)= std(delta_mfceps);\n",
      "        currentFrameIndex = currentFrameIndex + 1;\n",
      "    end\n",
      "\n",
      "    features = [MFCC_mean MFCC_delta_mean ]; % In this case, we'll only store the MFCC and delta-MFCC means\n",
      "    % NOTE: You might want to toss out the FIRST MFCC coefficient and delta-coefficient since it's much larger than \n",
      "    others and only describes the total energy of the signal.\n",
      "\n",
      "You can include this code inside of your frame-hopping loop to extract the MFCC-values for each frame. \n",
      "\n",
      "Once MFCCs per frame have been calculated, consider how they can be used as features for expanding the k-NN classification and try implementing it!\n",
      "\n",
      "Extract the mean of the 12 MFCCs (coefficients 1-12, do not use the \"0th\" coefficient) for each onset using the code that you wrote. Add those to the feature vectors, along with zero crossing and centroid. We should now have 14 features being extracted - this is starting to get \"real world\"! With this simple example (and limited collection of audio slices, you probably won't notice a difference - but at least it didn't break, right?) Try it with the some other audio to truly appreciate the power of timbral classification. \n",
      "\n",
      "\n",
      "\n",
      "SECTION 3 CROSS VALIDATION\n",
      "--------------------------\n",
      "\n",
      "You'll need some of this code and information to calculate your accuracy rate on your classifiers.\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "Let's say we have 10-fold cross validation...\n",
      "\n",
      "1. Divide test set into 10 random subsets.\n",
      "2. 1 test set is tested using the classifier trained on the remaining 9.\n",
      "3. We then do test/train on all of the other sets and average the percentages. \n",
      "\n",
      "To achieve the first step (divide our training set into k disjoint subsets), use the function [Kfold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) in the scikit.learn cross_validation package.\n",
      "\n",
      "    K-Folds cross validation iterator.\n",
      "    Provides train/test indices to split data in train test sets. Split dataset into k consecutive folds (without shuffling).\n",
      "\n",
      " You can visit the scikit.learn documentation to look at all the other options. This code is also posted as a template in \n",
      " `/usr/ccrma/courses/mir2014/Toolboxes/crossValidationTemplate.py`\n",
      "     "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import cross_validation\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def crossValidateKNN(features, labels):\n",
      "    \"\"\"\n",
      "    This code is provided as a template for your cross-validation\n",
      "    computation. Pass into the variables \"features\", \"labels\" your own data. \n",
      "\n",
      "    As well, you can replace the code in the \"BUILD\" and \"EVALUATE\" sections\n",
      "    to be useful with other types of Classifiers.\n",
      "    \"\"\"\n",
      "    #\n",
      "    # CROSS VALIDATION \n",
      "    # The features array is arranged as rows of instances, columns of features in our training set.\n",
      "    numInstances, numFeatures = features.shape\n",
      "    numFolds = min(10, numInstances) # how many cross-validation folds do you want - (default=10)\n",
      "    # divide test set into 10 random subsets\n",
      "    indices = cross_validation.KFold(numInstances, n_folds = numFolds)\n",
      "\n",
      "    errors = np.empty(numFolds)\n",
      "    for foldIndex, (train_index, test_index) in enumerate(indices):\n",
      "        # SEGMENT DATA INTO FOLDS\n",
      "        print('Fold: %d' % foldIndex) \n",
      "        print(\"TRAIN: %s\" % train_index)\n",
      "        print(\"TEST: %s\" % test_index)\n",
      "    \n",
      "        # SCALE\n",
      "        scaler = preprocessing.MinMaxScaler(feature_range = (-1, 1))\n",
      "        trainingFeatures = scaler.fit_transform(features.take(train_index, 0))\n",
      "        # BUILD NEW MODEL - ADD YOUR MODEL BUILDING CODE HERE...\n",
      "        model = KNeighborsClassifier(n_neighbors = 3)\n",
      "        model.fit(trainingFeatures, labels.take(train_index, 0))\n",
      "        # RESCALE TEST DATA TO TRAINING SCALE SPACE\n",
      "        testingFeatures = scaler.transform(features.take(test_index, 0))\n",
      "        # EVALUATE WITH TEST DATA - ADD YOUR MODEL EVALUATION CODE HERE\n",
      "        model_output = model.predict(testingFeatures)\n",
      "        print(\"KNN prediction %s\" % model_output) # Debugging.\n",
      "        # CONVERT labels(test,:) LABELS TO SAME FORMAT TO COMPUTE ERROR \n",
      "        labels_test = labels.take(test_index, 0)\n",
      "        # COUNT ERRORS. matches is a boolean array, taking the mean does the right thing.\n",
      "        matches = model_output != labels_test\n",
      "        errors[foldIndex] = matches.mean()\n",
      "    print('cross validation error: %f' % errors.mean())\n",
      "    print('cross validation accuracy: %f' % (1.0 - errors.mean()))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}