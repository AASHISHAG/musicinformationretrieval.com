{
 "metadata": {
  "name": "",
  "signature": "sha256:1ffd98b7c0e2505fce2d22c46232fa4cd491976740e5a8d1c6eec175cb6f3566"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab 3\n",
      "=====\n",
      "\n",
      "PURPOSE\n",
      "Sometimes, an unsupervised learning technique is preferred.  Perhaps you do not have access to adequate training data.  Or perhaps the classifications for the training data's labels events are not completely clear.  Or perhaps you just want to quickly sort real-world, unseen, data into groups based on it's feature similarity.   Regardless of your situation, clustering is a great option!\n",
      "\n",
      "Section 1: Clustering\n",
      "Now we're going to try clustering with a familiar bunch of audio files and code.  Sorry, the simple drum loop is going to make an appearance again.  However, once we prove that it works - you can experiment with other audio collections that are posted.  \n",
      "\n",
      "Create a new .m file for your code.   \n",
      "\n",
      "Load simpleLoop.wav.  (Sorry - we'll use other audio files soon!  It's best to start simple - because if it don't work for this file, we have a problem.)\n",
      "\n",
      "Segment this file into 100ms frames based on the onsets.\n",
      "\n",
      "Now, feature extract the frames  using only zero crossing and centroid.  Store the feature values in one matrix for both the kick and the snares\u2026 remember, we don't care about the labels with clustering - we just want to create  some clustered groups of data.\n",
      "\n",
      "Scale the features (using the scale function) from -1 to 1.   (See Lab 2 if you need a reminder.)\n",
      "\n",
      "It's cluster time!  We're using NETLAB's implementation of the kmeans algorithm.  \n",
      "\n",
      "Use the kmeans algorithm to create clusters of your feature.   kMeans will output 2 things of interest to you:  \n",
      "(1) The center-points of clusters.  You can use the coordinates of the center of the cluster to measure the distance of any point from the center.  This not only provides you with a distance metric of how \"good\" a point fits into a given cluster, but this allows you to sort by the points which are closest to the center of a given frame!  Quite useful.  \n",
      "\n",
      "(2) Each point will be assigned a label, or cluster #.   You can then use this label to produce a transcription, do creative stuff, or further train another downstream classifier.\n",
      "\n",
      "Attention:\n",
      "There are 2 functions called kmeans - one from the CATBox and another from Netlab.  You should be using the one from Netlab.  Verify that you are by typing which kmeans   in your command line to verify...\n",
      "\n",
      "Here's the help function for kmeans: \n",
      "\n",
      "> help kmeans\n",
      "\n",
      " KMEANS\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Trains a k means cluster model.\n",
      "\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Description\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CENTRES = KMEANS(CENTRES, DATA, OPTIONS) uses the batch K-means\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0algorithm to set the centres of a cluster model. The matrix DATA\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0represents the data which is being clustered, with each row\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0corresponding to a vector. The sum of squares error function is used.\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0The point at which a local minimum is achieved is returned as\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CENTRES.  The error value at that point is returned in OPTIONS(8).\n",
      "\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[CENTRES, OPTIONS, POST, ERRLOG] = KMEANS(CENTRES, DATA, OPTIONS)\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0also returns the cluster number (in a one-of-N encoding) for each\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0data point in POST and a log of the error values after each cycle in\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ERRLOG.    The optional parameters have the following\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0interpretations.\n",
      "\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0OPTIONS(1) is set to 1 to display error values; also logs error\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0values in the return argument ERRLOG. If OPTIONS(1) is set to 0, then\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0only warning messages are displayed.  If OPTIONS(1) is -1, then\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nothing is displayed.\n",
      "\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0OPTIONS(2) is a measure of the absolute precision required for the\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0value of CENTRES at the solution.  If the absolute difference between\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0the values of CENTRES between two successive steps is less than\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0OPTIONS(2), then this condition is satisfied.\n",
      "\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0OPTIONS(3) is a measure of the precision required of the error\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0function at the solution.  If the absolute difference between the\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0error functions between two successive steps is less than OPTIONS(3),\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0then this condition is satisfied. Both this and the previous\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0condition must be satisfied for termination.\n",
      "\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0OPTIONS(14) is the maximum number of iterations; default 100.\n",
      "\n",
      "     Now, simply put, here are some examples of how you use it: \n",
      "\n",
      "         % Initialize # of clusters that you want to find and their initial conditions.\n",
      "         numCenters = 2;           % the size of the initial centers; this is passed to k-means to determine the value of k.\n",
      "         numFeatures = 2;        %  replace the \"2\" with however many features you have extracted\n",
      "         centers = zeros(numCenters , numFeatures );           % inits center points to 0\n",
      "         \u00a0\n",
      "         % setup vector of options for kmeans trainer\n",
      "         options(1) = 1; \n",
      "         options(5) = 1;\n",
      "         options(14) = 50;   % num of steps to wait for convergence\n",
      "         \u00a0\n",
      "         % train centers from data\n",
      "         [centers,options,post] = kmeans(centers , your_feature_data_matrix , options);\n",
      "         \u00a0\n",
      "         %Output: \n",
      "         %     Centers contains the center coordinates of the clusters - we can use this to calculate the distance for each point      \n",
      "                  in the distance to the cluster center.\n",
      "                  %     Post contains the assigned cluster number for each point in your feature matrix.  (from 1 to k) \n",
      "\n",
      "Write a script to list which audio slices (or audio files) were categorized as Cluster # 1.  Do the same or Cluster # 2.  Do the clusters make sense?    Now, modify the script to play the audio slices that in each cluster - listening to the clusters will help us build intuition of what's in each cluster.  \n",
      "\n",
      "Repeat this clustering (steps 3-7), and listening to the contents of the clusters with CongaGroove-mono.wav.   \n",
      "\n",
      "Repeat this clustering (steps 3-7) using the CongaGroove and 3 clusters.  Listen to the results.  Try again with 4 clusters.  Listen to the results.  (etc, etc\u2026)\n",
      "\n",
      "Once you complete this, try out some of the many, many other audio loops in the audio loops. (Located In audio\\Miscellaneous Loops Samples and SFX)\n",
      "\n",
      "Let's add MFCCs to the mix.  Extract the mean of the 12 MFCCs (coefficients 1-12, do not use the \"0th\" coefficient) for each onset using the code that you wrote.  Add those to the feature vectors, along with zero crossing and centroid.  We should now have 14 features being extracted - this is started to get \"real world\"!   With this simple example (and limited collection of audio slices, you probably won't notice a difference - but at least it didn't break, right?)  Let's try it with the some other audio to truly appreciate the power of timbral clustering.\n",
      "\n",
      "BONUS (ONLY IF YOU HAVE EXTRA TIME\u2026)\n",
      "Now that we can take ANY LOOP, onset detect, feature extract, and cluster it, let's have some fun.   \n",
      "Choose any audio file from our collection and use the above techniques break it up into clusters.   \n",
      "Listen to those clusters.\n",
      "\n",
      "Some rules of thumb: since you need to pick the number of clusters ahead of time, listen to your audio files first.  \n",
      "You can break a drum kit or percussion loop into 3 - 6 clusters for it to segment well.  More is OK too.\n",
      "Musical loops: 3-6 clusters should work nicely.  \n",
      "Songs - lots of clusters for them to segment well.  Try 'em out!\n",
      "\n",
      "BONUS (ONLY IF YOU REALLY HAVE EXTRA TIME\u2026)\n",
      "Review your script that PLAYs all of the audio files that were categorized as Cluster # 1 or Cluster # 2.  \n",
      "Now, modify your script to play and plot the audio files which are closest to the center of your clusters.\n",
      "\n",
      "This hopefully provides you with which files are representative of your cluster.  \n",
      "\n",
      "Helpful Commands for sorting or measuring distance: \n",
      "\n",
      "d = dist2( featureVector1 ,featureVector2 )   % measures the Euclidean distance betw/ point 1 and point 1\n",
      "\n",
      "DIST2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Calculates squared distance between two sets of points.\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Description\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0D = DIST2(X, C) takes two matrices of vectors and calculates the\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0squared Euclidean distance between them.  Both matrices must be of\n",
      "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0the same column dimension.  If X has M rows and N columns, and C has\n",
      " \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0L rows and N columns, then the result has M rows and L columns.  The\n",
      "  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0I, Jth entry is the  squared distance from the Ith row of X to the\n",
      "   \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Jth row of C.\n",
      "\n",
      "[y,ind] = sort(    ) \n",
      ">> help sort\n",
      "SORT   Sort in ascending or descending order.\n",
      "For vectors, SORT(X) sorts the elements of X in ascending order.\n",
      "For matrices, SORT(X) sorts each column of X in ascending order.\n",
      "For N-D arrays, SORT(X) sorts the along the first non-singleton\n",
      "dimension of X. When X is a cell array of strings, SORT(X) sorts\n",
      "the strings in ASCII dictionary order.\n",
      "\n",
      "Y = SORT(X,DIM,MODE)\n",
      "has two optional parameters.  \n",
      "DIM selects a dimension along which to sort.\n",
      "MODE selects the direction of the sort\n",
      "'ascend' results in ascending order\n",
      "    'descend' results in descending order\n",
      "        The result is in Y which has the same shape and type as X.\n",
      "\n",
      "[Y,I] = SORT(X,DIM,MODE) also returns an index matrix I.\n",
      "If X is a vector, then Y = X(I).  \n",
      "If X is an m-by-n matrix and DIM=1, then\n",
      "for j = 1:n, Y(:,j) = X(I(:,j),j); end\n",
      "\n",
      "When X is complex, the elements are sorted by ABS(X).  Complex\n",
      "matches are further sorted by ANGLE(X).\n",
      "\n",
      "When more than one element has the same value, the order of the\n",
      "elements are preserved in the sorted result and the indexes of\n",
      "equal elements will be ascending in any index matrix.\n",
      "\n",
      "Example: If X = [3 7 5\n",
      "0 4 2]\n",
      "\n",
      "then sort(X,1) is [0 4 2  and sort(X,2) is [3 5 7\n",
      "3 7 5]                   0 2 4];\n",
      "\u00a0\n",
      "[y,ind] = sortrows (  ) \n",
      "\u00a0\n",
      "SORTROWS Sort rows in ascending order.\n",
      "Y = SORTROWS(X) sorts the rows of the matrix X in ascending order as a\n",
      "group. X is a 2-D numeric or char matrix. For a char matrix containing\n",
      "strings in each row, this is the familiar dictionary sort.  When X is\n",
      "complex, the elements are sorted by ABS(X). Complex matches are further\n",
      "sorted by ANGLE(X).  X can be any numeric or char class. Y is the same\n",
      "size and class as X.\n",
      "\n",
      "SORTROWS(X,COL) sorts the matrix based on the columns specified in the\n",
      "vector COL.  If an element of COL is positive, the corresponding column\n",
      "in X will be sorted in ascending order; if an element of COL is negative,\n",
      "the corresponding column in X will be sorted in descending order. For \n",
      "example, SORTROWS(X,[2 -3]) sorts the rows of X first in ascending order \n",
      "for the second column, and then by descending order for the third\n",
      "column.\n",
      "\n",
      "[Y,I] = SORTROWS(X) and [Y,I] = SORTROWS(X,COL) also returns an index \n",
      "matrix I such that Y = X(I,:).\n",
      "[y,ind] = sortrows (featureData_from_a_particular_cluster, clusterNum)\n",
      "    \u00a0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}