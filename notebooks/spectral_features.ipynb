{
 "metadata": {
  "name": "",
  "signature": "sha256:3e21fdc85c366449b7dce6c5e6e1aedf32f06eb49d2a03663f8375eaccbea6e3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Spectral Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For classification, we're going to be using the new features in our arsenal: cherishing those \"spectral moments\" (centroid, bandwidth, skewness, kurtosis) and also examining other spectral statistics."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Training Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we want to analyze and feature extract a small collection of audio samples - storing their feature data as our \"training data\".  The commands below read all of the drum example .wav files from the MIR web site into an array, `snare_file_list`.  \n",
      "\n",
      "Let's define a function to retrieve a list of URLs from a text file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "\n",
      "def process_corpus(corpus_URL):\n",
      "    \"\"\"Read a list of files to process from the text file at corpusURL. Return a list of URLs\"\"\" \n",
      "    # Open and read each line\n",
      "    url_list_text_data = urllib2.urlopen(corpus_URL) # it's a file like object and works just like a file\n",
      "    for file_URL in url_list_text_data: # files are iterable\n",
      "        yield file_URL.rstrip()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use these commands to read in a list of filenames (samples) in a directory, replacing the URL with a URL to a list of URLs (one per line) indicating where the audio / drum samples are stored."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snares_URL = \"https://ccrma.stanford.edu/workshops/mir2014/SnareCorpus.txt\"\n",
      "snare_file_list = [audio_file_URL for audio_file_URL in process_corpus(snares_URL)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kicks_URL = \"https://ccrma.stanford.edu/workshops/mir2014/KickCorpus.txt\"\n",
      "kick_file_list = [audio_file_URL for audio_file_URL in process_corpus(kicks_URL)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To access the filenames contained in the array, use the square brackets [ ] to get to the element that you want to access. For example, to access the text URL file name of the first file in the list, you would type:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snare_URL = snare_file_list[0]\n",
      "snare_URL"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "'https://ccrma.stanford.edu/workshops/mir2014/audio/drum%20samples/snares/SNARE_01_01.WAV'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When we feature extract a sample collection, we need to sequentially access audio files, segment them (or not), and feature extract them.  Loading a lot of audio files into memory is not always a feasible or desirable operation, so you will create a loop which loads an audio file, feature extracts it, and closes  the audio file.  Note that the only information that we retain in memory are the features that are extracted.\n",
      "\n",
      "Create a loop which reads in an audio file, extracts the zero crossing rate, and some spectral statistics. You can use the \"in\" operator to retrieve each audio file URL from process_corpus(), as used above. The feature information for each audio file (the \"feature vector\") should be stored as a feature array, with columns being the features and rows for each file. For example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "number_of_files = 20\n",
      "number_of_features = 5\n",
      "features_snare = np.zeros([number_of_files, number_of_features])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " For example, when populated, features_snare might look like:\n",
      " \n",
      "    features_snare =\n",
      "\n",
      "             0.5730    1.9183    2.9713    0.0004  0.0002\n",
      "             0.4750    1.4834    2.4463    0.0004  0.0012\n",
      "             0.5900    2.2857    3.1788    0.0003  0.0041\n",
      "             0.5090    1.6622    2.6369    0.0004  0.0051\n",
      "             0.4860    1.4758    2.2085    0.0004  0.0021\n",
      "             0.6060    2.2119    3.2798    0.0004  0.0651\n",
      "             0.4990    2.0607    2.7654    0.0004  0.0721\n",
      "             0.6360    2.3153    3.0256    0.0003  0.0221\n",
      "             0.5490    2.0137    3.0342    0.0004  0.0016\n",
      "             0.5900    2.2857    3.1788    0.0003  0.0012\n",
      " \n",
      " Within your loop, here's a reminder how to read in your wav files, using an array of audio file URLs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "from essentia.standard import MonoLoader\n",
      "\n",
      "sample_rate = 44100\n",
      "urllib.urlretrieve(snare_URL, filename='/tmp/localfile.wav')\n",
      "audio = MonoLoader(filename = '/tmp/localfile.wav', sampleRate = sample_rate)()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's an example of how to feature extract the first frame from the current audio file, using Essentia's [ZeroCrossingRate](http://essentia.upf.edu/documentation/reference/streaming_ZeroCrossingRate.html) and [CentralMoments](http://essentia.upf.edu/documentation/reference/std_CentralMoments.html) classes..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from essentia.standard import ZeroCrossingRate, CentralMoments, Spectrum, Windowing, Centroid\n",
      "zcr = ZeroCrossingRate()\n",
      "hamming_window = Windowing(type = 'hamming') # we need to window the frame to avoid FFT artifacts.\n",
      "spectrum = Spectrum()\n",
      "central_moments = CentralMoments()\n",
      "spectral_centroid = Centroid()\n",
      "\n",
      "frame_size = 0.100 * sample_rate   # 100ms\n",
      "file_index = 0 # to process a single file, this will be your file loop iteration\n",
      "\n",
      "current_frame = audio[0 : frame_size]\n",
      "features_snare[file_index, 0] = zcr(current_frame)\n",
      "spectral_magnitude = spectrum(hamming_window(current_frame))\n",
      "centroid = spectral_centroid(spectral_magnitude)\n",
      "\n",
      "# Spectral centroid in normalised bandwidth (i.e fraction of Nyquist frequency).\n",
      "centroid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.22001729905605316"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Centroid in spectral coefficient indices.\n",
      "centroid * spectral_magnitude.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "485.3581617176533"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Essentia can make things seem a little magical, here is how the centroid would be calculated:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalize the spectral magnitude\n",
      "norm_spectral_mag = spectral_magnitude / np.sum(spectral_magnitude)\n",
      "# Make index vector\n",
      "indices = np.arange(norm_spectral_mag.shape[0])\n",
      "\n",
      "# Centroid, in spectral coefficient indices.\n",
      "my_centroid = np.sum(norm_spectral_mag * indices)\n",
      "my_centroid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "485.13728477961195"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we use the magnitude of the spectrum to compute a 5 element array, corresponding to:\n",
      "# zeroth central moment (1.0)\n",
      "# first central moment (Not the centroid, which is the first moment!)\n",
      "# bandwidth\n",
      "# skew\n",
      "# kurtosis.\n",
      "spectral_moments = central_moments(spectral_magnitude)\n",
      "spectral_moments"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([ 1.        ,  0.        ,  0.04079479,  0.00723291,  0.00497805], dtype=float32)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Remember the zeroth column of features_snare is the ZCR.\n",
      "features_snare[file_index, 1] = centroid\n",
      "features_snare[file_index, 2:5] = spectral_moments[2:5]\n",
      "features_snare[file_index]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "array([ 0.1292517 ,  0.2200173 ,  0.04079479,  0.00723291,  0.00497805])"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4.  First, extract all of the feature data for the kick drums and store it in a feature array.  (For my example, above, I'd put it in \"features_kick\")\n",
      "\n",
      "5.  Next, extract all of the feature data for the snares, storing them in a different array. \n",
      "Again, the kick and snare features should be separated in two different arrays!\n",
      " \n",
      "OK, no more help.  The rest is up to you!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}